optimizer:
  name: adam
  lr: 1.e-4
  weight_decay: 0

train:
  epoch: 70
  batch_size: 512
  test_step: 1 # evaluate per {test_step} epochs
  reproducible: true
  seed: 2023
  save_model: true
  trainer: maerec_trainer
  log_loss: false

test:
  metrics: [recall, ndcg] # choose in {ndcg, recall, precision, mrr}
  k: [5, 10, 20,30] # top-k
  batch_size: 512 # How many users per batch during validation

data:
  type: sequential # choose in {general_cf, multi_behavior, sequential, social}
  name: 1
  seq_aug: true

target_data:
  type: sequential # choose in {general_cf, multi_behavior, sequential, social}
  name: 1
  seq_aug: true


model:
  name: maerec # case-insensitive
  con_batch: 2048
  max_seq_len: 50
  num_reco_neg: 40
  reg: 1.e-8
  ssl_reg: 1.e-3
  embedding_size: 128
  mask_depth: 3
  path_prob: 0.5
  num_attention_heads: 4
  num_gcn_layers: 2
  num_trm_layers: 2
  num_mask_cand: 50
  mask_steps: 100
  eps: 0.2
  attention_probs_dropout_prob: 0.3
  hidden_dropout_prob: 0.3


diffusion:
  mean_type: 'x0'  # MeanType for diffusion: x0, eps
  steps: 150  # diffusion steps
  noise_schedule: 'cosine'  # the schedule for noise generating
  noise_scale: 0.5  # noise scale for noise generating
  noise_min: 0.1  # noise lower bound for noise generating
  noise_max: 0.3  # noise upper bound for noise generating
  sampling_noise: false  # sampling with noise or not
  sampling_steps: 0  # steps of the forward process during inference
  reweight: true  # assign different weight to different timestep or not
  interval: 1
  time_type: 'cat'  # cat or add
  dims: '[2000]'  # the dims for the DNN
  norm: false  # Normalize the input or not
  emb_size: 10  # timestep embedding size
  
tune:
  enable: false # Whether to enable grid search to search for optimal hyperparameters
